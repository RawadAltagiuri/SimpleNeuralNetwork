{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the data, it is a csv file obtained from Kaggle at https://www.kaggle.com/competitions/digit-recognizer\n",
    "data = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "\n",
    "#This will be the development set, it is the first 1000 examples\n",
    "data_dev = data[0:1000].T # Transpose the data\n",
    "Y_dev = data_dev[0] # This is the label\n",
    "X_dev = data_dev[1:n] # This is the pixel data\n",
    "X_dev = X_dev / 255. # Normalize the data to be between 0 and 1\n",
    "\n",
    "#This will be the training set, it is the rest of the examples, we use the training set for gradient descent\n",
    "data_train = data[1000:m].T # Transpose the data\n",
    "Y_train = data_train[0] # This is the label\n",
    "X_train = data_train[1:n] # This is the pixel data\n",
    "X_train = X_train / 255. # Normalize the data to be between 0 and 1\n",
    "_,m_train = X_train.shape # m is the number of examples, n is the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the first data point\n",
    "plt.imshow(X_dev[:, 1].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        \"\"\"\n",
    "        Initializes a neural network layer with the specified number of inputs and outputs.\n",
    "        \n",
    "        Parameters:\n",
    "        n_input (int): The number of input features to the layer.\n",
    "        n_output (int): The number of output features from the layer.\n",
    "        \n",
    "        Attributes:\n",
    "        W (ndarray): The weight matrix for the layer.\n",
    "        b (ndarray): The bias vector for the layer.\n",
    "        \"\"\"\n",
    "        self.W = np.random.rand(n_output, n_input) - 0.5\n",
    "        self.b = np.zeros((n_output, 1)) - 0.5\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computes the forward pass through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        input (ndarray): Input matrix of shape (n_input, m), where m is the number of examples.\n",
    "        \n",
    "        Returns:\n",
    "        output (ndarray): Output matrix of shape (n_output, m). It is the result of the forward pass.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.W, input) + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backwardSoft(self, prediction, Y_one_hot, prevReLu, learningRate, m):\n",
    "        \"\"\"\n",
    "        Performs backpropagation for the output layer using the softmax activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        prediction (ndarray): Predicted probabilities for each class, shape (n_output, m). These are the output of the softmax activation function.\n",
    "        Y_one_hot (ndarray): One-hot encoded true labels, shape (n_output, m). \n",
    "        prevReLu (ndarray): Output of the previous layer after applying ReLU activation, shape (n_input, m).\n",
    "        learningRate (float): Learning rate used for updating the weights and biases.\n",
    "        m (int): Number of training examples (batch size).\n",
    "        \n",
    "        Returns:\n",
    "        dPrediction (ndarray): Gradient of the loss with respect to the predictions, shape (n_output, m).\n",
    "        \"\"\"\n",
    "        self.dPrediction = prediction - Y_one_hot\n",
    "        dW = (1/ m) * self.dPrediction.dot(prevReLu.T)\n",
    "        db = (1/ m) * np.sum(self.dPrediction)\n",
    "        self.W = self.W - learningRate * dW\n",
    "        self.b = self.b - learningRate * db\n",
    "        return self.dPrediction\n",
    "    \n",
    "    def backwardReLu(self, NextLayer, ReLU, Y_one_hot, learningRate, m):\n",
    "        \"\"\"\n",
    "        Performs backpropagation for a hidden layer using the ReLU activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        NextLayer (Layer): The next layer in the network, used to access its weights and gradients.\n",
    "        ReLU (ReLU): The ReLU activation object, used to compute the derivative of ReLU.\n",
    "        Y_one_hot (ndarray): One-hot encoded true labels, shape (n_output, m).\n",
    "        learningRate (float): Learning rate used for updating the weights and biases.\n",
    "        m (int): Number of training examples (batch size).\n",
    "        \n",
    "        Returns:\n",
    "        dOutput (ndarray): Gradient of the loss with respect to the layer's output, shape (n_output, m).\n",
    "        \"\"\"\n",
    "        self.dOutput = NextLayer.W.T.dot(NextLayer.dPrediction) * ReLU.derivative(self.output)\n",
    "        dW = (1/ m) * self.dOutput.dot(self.input.T)\n",
    "        db = (1/ m) * np.sum(self.dOutput)\n",
    "        self.W = self.W - learningRate * dW\n",
    "        self.b = self.b - learningRate * db\n",
    "        return self.dOutput\n",
    "        \n",
    "class ReLU:\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computes the forward pass through the ReLU activation function.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0, input)\n",
    "        return self.output\n",
    "    \n",
    "    def derivative(self ,Layer_output):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the ReLU activation function.\n",
    "        \"\"\"\n",
    "        return Layer_output > 0\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computes the forward pass through the softmax activation function.\n",
    "        \"\"\"\n",
    "        self.output = np.exp(input) / sum(np.exp(input))\n",
    "        return self.output\n",
    "    \n",
    "def OneHotEncode(Y):\n",
    "    \"\"\"\n",
    "    One hot encode the labels.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def getPrediction(output):\n",
    "    \"\"\"\n",
    "    Get the prediction from the output of the softmax layer.\n",
    "    \"\"\"\n",
    "    return np.argmax(output, axis=0)\n",
    "\n",
    "def getAccuracy(prediction, Y):\n",
    "    \"\"\"\n",
    "    Get the accuracy of the model.\n",
    "    \"\"\"\n",
    "    return np.sum(prediction == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy:  0.09814634146341464 epoch:  0\n",
      " Accuracy:  0.47897560975609754 epoch:  50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3264\\3296179425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Apply the ReLU activation function to the output of the first layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0moutput1_ReLU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Pass the output of ReLU through the second layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3264\\4220006145.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \"\"\"\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the first layer of the neural network\n",
    "# Input size is 784 (e.g., for a 28x28 pixel image), output size is 10 (e.g., for 10 classes)\n",
    "layer1 = Layer(784, 10)\n",
    "\n",
    "# Initialize the ReLU activation function object\n",
    "relu = ReLU()\n",
    "\n",
    "# Initialize the second layer of the neural network\n",
    "# Input size is 10 (from the output of layer1), output size is 10 (for 10 classes)\n",
    "layer2 = Layer(10, 10)\n",
    "\n",
    "# Initialize the Softmax activation function object\n",
    "softmax = Softmax()\n",
    "\n",
    "# Set the learning rate for the gradient descent updates\n",
    "learningRate = 0.1\n",
    "\n",
    "# Training loop: Run for 10,000 epochs (iterations)\n",
    "for epoch in range(10000):\n",
    "    # Forward pass through the network\n",
    "\n",
    "    # Pass the input data (X_train) through the first layer\n",
    "    output1 = layer1.forward(X_train)\n",
    "    \n",
    "    # Apply the ReLU activation function to the output of the first layer\n",
    "    output1_ReLU = relu.forward(output1)\n",
    "    \n",
    "    # Pass the output of ReLU through the second layer\n",
    "    output2 = layer2.forward(output1_ReLU)\n",
    "    \n",
    "    # Apply the Softmax activation function to the output of the second layer\n",
    "    output2_Softmax = softmax.forward(output2)\n",
    "    \n",
    "    # Get the predicted class labels from the softmax probabilities\n",
    "    predection = getPrediction(output2_Softmax)\n",
    "    \n",
    "    # Calculate the accuracy of the predictions by comparing them to the true labels (Y_train)\n",
    "    accuracy = getAccuracy(predection, Y_train)\n",
    "    \n",
    "    # Print every 50 epochs\n",
    "    if epoch % 50 == 0:\n",
    "        print(' Accuracy: ', accuracy, 'epoch: ', epoch)\n",
    "\n",
    "    # Backward pass through the network\n",
    "    \n",
    "    # Compute the gradient of the loss with respect to the predictions (output2_Softmax)\n",
    "    dPrediction = layer2.backwardSoft(output2_Softmax, OneHotEncode(Y_train), output1_ReLU, learningRate, m)\n",
    "    \n",
    "    # Compute the gradient of the loss with respect to the output of layer1\n",
    "    dOutput = layer1.backwardReLu(layer2, relu, OneHotEncode(Y_train), learningRate, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "# Now we test the model\n",
    "\n",
    "output1 = layer1.forward(X_dev)\n",
    "output1_ReLU = relu.forward(output1)\n",
    "output2 = layer2.forward(output1_ReLU)\n",
    "output2_Softmax = softmax.forward(output2)\n",
    "predection = getPrediction(output2_Softmax)\n",
    "accuracy = getAccuracy(predection, Y_dev)\n",
    "print('Accuracy on the 3rd data point:', accuracy)\n",
    "for i in range(X_dev.shape[1]):\n",
    "    data_sample_index = i\n",
    "    plt.imshow(X_dev[:, data_sample_index].reshape(28, 28), cmap='cool')\n",
    "    plt.show()\n",
    "    print('Prediction:', predection[data_sample_index])\n",
    "    print('Label:', Y_dev[data_sample_index])\n",
    "    # wait 1 second\n",
    "    plt.pause(1)\n",
    "    # clear the current figure\n",
    "    plt.clf()\n",
    "    os.system('clear')\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random image\n",
    "random_image = np.random.rand(28*28)\n",
    "plt.imshow(random_image.reshape(28, 28), cmap='rainbow')\n",
    "plt.show()\n",
    "#make it into a column\n",
    "random_image = random_image.reshape(784, 1)\n",
    "#pass it through the network\n",
    "output1 = layer1.forward(random_image)\n",
    "output1_ReLU = relu.forward(output1)\n",
    "output2 = layer2.forward(output1_ReLU)\n",
    "output2_Softmax = softmax.forward(output2)\n",
    "predection = getPrediction(output2_Softmax)\n",
    "print('Prediction:', predection[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
